{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamA333/PowerBI-Dashboards/blob/main/YOLOv5_Classification_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLOv5 Classification Tutorial\n",
        "\n",
        "YOLOv5 supports classification tasks too. This is the official YOLOv5 classification notebook tutorial. YOLOv5 is maintained by [Ultralytics](https://github.com/ultralytics/yolov5).\n",
        "\n",
        "This notebook covers:\n",
        "\n",
        "*   Inference with out-of-the-box YOLOv5 classification on ImageNet\n",
        "*  [Training YOLOv5 classification](https://blog.roboflow.com//train-YOLOv5-classification-custom-data) on custom data\n",
        "\n",
        "*Looking for custom data? Explore over 66M community datasets on [Roboflow Universe](https://universe.roboflow.com).*\n",
        "\n",
        "This notebook was created with Google Colab. [Click here](https://colab.research.google.com/drive/1FiSNz9f_nT8aFtDEU3iDAQKlPT8SCVni?usp=sharing) to run it."
      ],
      "metadata": {
        "id": "5GYQX3of4QiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Pull in respective libraries to prepare the notebook environment."
      ],
      "metadata": {
        "id": "-PJ8vlYXbWtN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pIM7fOwm8A7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6d71a1-f962-41db-8089-63568ae36313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v7.0-245-g3d8f004 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.9/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Infer on ImageNet\n",
        "\n",
        "To demonstrate YOLOv5 classification, we'll leverage an already trained model. In this case, we'll download the ImageNet trained models pretrained on ImageNet using YOLOv5 Utils."
      ],
      "metadata": {
        "id": "i_DrUi2nmF40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.downloads import attempt_download\n",
        "\n",
        "p5 = ['n', 's', 'm', 'l', 'x']  # P5 models\n",
        "cls = [f'{x}-cls' for x in p5]  # classification models\n",
        "\n",
        "for x in cls:\n",
        "    attempt_download(f'weights/yolov5{x}.pt')"
      ],
      "metadata": {
        "id": "o2scLEh6EYnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can infer on an example image from the ImageNet dataset."
      ],
      "metadata": {
        "id": "Fn2_a38DmZ2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download example image\n",
        "import requests\n",
        "image_url = \"https://i.imgur.com/OczPfaz.jpg\"\n",
        "img_data = requests.get(image_url).content\n",
        "with open('bananas.jpg', 'wb') as handler:\n",
        "    handler.write(img_data)"
      ],
      "metadata": {
        "id": "L9objhVHnS-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Infer using classify/predict.py\n",
        "!python classify/predict.py --weights ./weigths/yolov5s-cls.pt --source bananas.jpg"
      ],
      "metadata": {
        "id": "qqxF5pHCrLd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output, we can see the ImageNet trained model correctly predicts the class `banana` with `0.95` confidence."
      ],
      "metadata": {
        "id": "yQmj7IXqo3kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. (Optional) Validate\n",
        "\n",
        "Use the `classify/val.py` script to run validation for the model. This will show us the model's performance on each class.\n",
        "\n",
        "First, we need to download ImageNet."
      ],
      "metadata": {
        "id": "5EosQzyDCk3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # WARNING: takes ~20 minutes\n",
        "# !bash data/scripts/get_imagenet.sh --val"
      ],
      "metadata": {
        "id": "HwAYptjCq-C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # run the validation script\n",
        "# !python classify/val.py --weights ./weigths/yolov5s-cls.pt --data ../datasets/imagenet"
      ],
      "metadata": {
        "id": "CoHdKXWc8hrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows accuracy metrics for the ImageNet validation dataset including per class accuracy."
      ],
      "metadata": {
        "id": "r2coOcIjuzCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train On Custom Data\n",
        "\n",
        "To train on custom data, we need to prepare a dataset with custom labels.\n",
        "\n",
        "To prepare custom data, we'll use [Roboflow](https://roboflow.com). Roboflow enables easy dataset prep with your team, including labeling, formatting into the right export format, deploying, and active learning with a `pip` package.\n",
        "\n",
        "If you need custom data, there are over 66M open source images from the community on [Roboflow Universe](https://universe.roboflow.com).\n",
        "\n",
        "(For more guidance, here's a detailed blog on [training YOLOv5 classification on custom data](https://blog.roboflow.com/train-YOLOv5-classification-custom-data).)\n",
        "\n",
        "\n",
        "Create a free Roboflow account, upload your data, and label.\n",
        "\n",
        "![](https://s4.gifyu.com/images/fruit-labeling.gif)"
      ],
      "metadata": {
        "id": "9bXHHYeVDCXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Custom Dataset\n",
        "\n",
        "Next, we'll export our dataset into the right directory structure for training YOLOv5 classification to load into this notebook. Select the `Export` button at the top of the version page, `Folder Structure` type, and `show download code`.\n",
        "\n",
        "The ensures all our directories are in the right format:\n",
        "\n",
        "```\n",
        "dataset\n",
        "â”œâ”€â”€ train\n",
        "â”‚Â Â  â”œâ”€â”€ class-one\n",
        "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_123.jpg\n",
        "â”‚Â Â  â””â”€â”€ class-two\n",
        "â”‚Â Â      â”œâ”€â”€ IMG_456.jpg\n",
        "â”œâ”€â”€ valid\n",
        "â”‚Â Â  â”œâ”€â”€ class-one\n",
        "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_789.jpg\n",
        "â”‚Â Â  â””â”€â”€ class-two\n",
        "â”‚Â Â      â”œâ”€â”€ IMG_101.jpg\n",
        "â”œâ”€â”€ test\n",
        "â”‚Â Â  â”œâ”€â”€ class-one\n",
        "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_121.jpg\n",
        "â”‚Â Â  â””â”€â”€ class-two\n",
        "â”‚Â Â      â”œâ”€â”€ IMG_341.jpg\n",
        "```\n",
        "\n",
        "![](https://i.imgur.com/BF9BNR8.gif)\n",
        "\n",
        "\n",
        "Copy and paste that snippet into the cell below."
      ],
      "metadata": {
        "id": "Cu6-lrukD6Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure we're in the right directory to download our custom dataset\n",
        "import os\n",
        "os.makedirs(\"../datasets/\", exist_ok=True)\n",
        "%cd ../datasets/"
      ],
      "metadata": {
        "id": "6IIgJbP7G6Th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ad6a60-efb2-44b9-bd91-ce57e992106d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"OLEusDLSXJg1Kbc171Xa\")\n",
        "project = rf.workspace(\"nmims-oawfg\").project(\"sign-language-detectiom\")\n",
        "dataset = project.version(1).download(\"folder\")\n"
      ],
      "metadata": {
        "id": "He6JwHIlG-W_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64913ada-f77a-464a-d10b-bffcd2449554"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Sign-Language-Detectiom-1 to folder:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81898/81898 [00:01<00:00, 76186.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Sign-Language-Detectiom-1 in folder:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26027/26027 [00:02<00:00, 10568.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the dataset name to the environment so we can use it in a system call later\n",
        "dataset_name = dataset.location.split(os.sep)[-1]\n",
        "os.environ[\"DATASET_NAME\"] = dataset_name"
      ],
      "metadata": {
        "id": "wLQbThFICpn4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train On Custom Data ðŸŽ‰\n",
        "Here, we use the DATASET_NAME environment variable to pass our dataset to the `--data` parameter.\n",
        "\n",
        "Note: we're training for 100 epochs here. We're also starting training from the pretrained weights. Larger datasets will likely benefit from longer training."
      ],
      "metadata": {
        "id": "-5z7Yv42FGrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../yolov5\n",
        "!python /content/yolov5/classify/train.py --model yolov5s-cls.pt --data $DATASET_NAME --epochs 100 --img 128 --pretrained weights/yolov5s-cls.pt"
      ],
      "metadata": {
        "id": "MXWTTN2BEaqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50266f15-82d0-4a4d-ef6e-853e242ec487"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '../yolov5'\n",
            "/datasets\n",
            "2023-11-23 10:41:45.786308: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-23 10:41:45.786378: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-23 10:41:45.786426: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5s-cls.pt, data=Sign-Language-Detectiom-1, epochs=100, batch_size=64, imgsz=128, nosave=False, cache=None, device=, workers=8, project=../content/yolov5/runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5s-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v7.0-245-g3d8f004 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir ../content/yolov5/runs/train-cls', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=128, width=128, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to yolov5s-cls.pt...\n",
            "100% 10.5M/10.5M [00:00<00:00, 115MB/s]\n",
            "\n",
            "Model summary: 149 layers, 4299299 parameters, 4299299 gradients, 10.6 GFLOPs\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\n",
            "Image sizes 128 train, 128 test\n",
            "Using 1 dataloader workers\n",
            "Logging results to \u001b[1m../content/yolov5/runs/train-cls/exp\u001b[0m\n",
            "Starting yolov5s-cls.pt training on Sign-Language-Detectiom-1 dataset with 99 classes for 100 epochs...\n",
            "\n",
            "     Epoch   GPU_mem  train_loss   test_loss    top1_acc    top5_acc\n",
            "     1/100    0.736G        3.17        5.54      0.0863       0.242: 100% 372/372 [00:43<00:00,  8.54it/s]\n",
            "     2/100    0.736G        2.13        5.59       0.153       0.239: 100% 372/372 [00:39<00:00,  9.53it/s]\n",
            "     3/100    0.736G        1.78         5.8       0.195       0.222: 100% 372/372 [00:41<00:00,  9.05it/s]\n",
            "     4/100    0.736G         1.6        5.77       0.198       0.222: 100% 372/372 [00:38<00:00,  9.56it/s]\n",
            "     5/100    0.736G        1.49           6       0.162       0.234: 100% 372/372 [00:40<00:00,  9.15it/s]\n",
            "     6/100    0.736G        1.42        5.92       0.196       0.223: 100% 372/372 [00:39<00:00,  9.41it/s]\n",
            "     7/100    0.736G        1.35        6.12       0.195       0.233: 100% 372/372 [00:39<00:00,  9.31it/s]\n",
            "     8/100    0.736G        1.31        6.21       0.195       0.219: 100% 372/372 [00:40<00:00,  9.22it/s]\n",
            "     9/100    0.736G        1.28        6.13       0.194       0.259: 100% 372/372 [00:39<00:00,  9.49it/s]\n",
            "    10/100    0.736G        1.25        6.11       0.183       0.243: 100% 372/372 [00:41<00:00,  9.01it/s]\n",
            "    11/100    0.736G        1.22        6.21       0.191        0.24: 100% 372/372 [00:38<00:00,  9.57it/s]\n",
            "    12/100    0.736G         1.2        6.17       0.195        0.26: 100% 372/372 [00:40<00:00,  9.10it/s]\n",
            "    13/100    0.736G        1.18        6.33       0.195       0.255: 100% 372/372 [00:38<00:00,  9.60it/s]\n",
            "    14/100    0.736G        1.16        6.23       0.196       0.246: 100% 372/372 [00:40<00:00,  9.09it/s]\n",
            "    15/100    0.736G        1.15        6.34       0.195       0.247: 100% 372/372 [00:38<00:00,  9.68it/s]\n",
            "    16/100    0.736G        1.13        6.24       0.195       0.278: 100% 372/372 [00:40<00:00,  9.27it/s]\n",
            "    17/100    0.736G        1.12         6.4       0.196       0.267: 100% 372/372 [00:37<00:00,  9.85it/s]\n",
            "    18/100    0.736G         1.1        6.47       0.195        0.24: 100% 372/372 [00:39<00:00,  9.35it/s]\n",
            "    19/100    0.736G        1.09        6.51       0.195       0.219: 100% 372/372 [00:37<00:00,  9.83it/s]\n",
            "    20/100    0.736G        1.09        6.47       0.195       0.226: 100% 372/372 [00:40<00:00,  9.25it/s]\n",
            "    21/100    0.736G        1.08        6.55       0.195       0.231: 100% 372/372 [00:39<00:00,  9.37it/s]\n",
            "    22/100    0.736G        1.07         6.5       0.194       0.244: 100% 372/372 [00:39<00:00,  9.32it/s]\n",
            "    23/100    0.736G        1.06        6.56       0.194       0.254: 100% 372/372 [00:38<00:00,  9.68it/s]\n",
            "    24/100    0.736G        1.05        6.54       0.194       0.252: 100% 372/372 [00:40<00:00,  9.26it/s]\n",
            "    25/100    0.736G        1.05        6.54       0.195       0.245: 100% 372/372 [00:38<00:00,  9.55it/s]\n",
            "    26/100    0.736G        1.04         6.6       0.194       0.247: 100% 372/372 [00:40<00:00,  9.24it/s]\n",
            "    27/100    0.736G        1.03        6.46       0.195       0.258: 100% 372/372 [00:40<00:00,  9.21it/s]\n",
            "    28/100    0.736G        1.03        6.47       0.195       0.243: 100% 372/372 [00:40<00:00,  9.26it/s]\n",
            "    29/100    0.736G        1.02        6.49       0.194       0.259: 100% 372/372 [00:41<00:00,  8.95it/s]\n",
            "    30/100    0.736G        1.01        6.61       0.195       0.253: 100% 372/372 [00:39<00:00,  9.47it/s]\n",
            "    31/100    0.736G        1.01        6.59       0.195       0.272: 100% 372/372 [00:41<00:00,  8.89it/s]\n",
            "    32/100    0.736G        1.01        6.61       0.195       0.255: 100% 372/372 [00:39<00:00,  9.44it/s]\n",
            "    33/100    0.736G           1        6.64       0.196       0.247: 100% 372/372 [00:41<00:00,  9.01it/s]\n",
            "    34/100    0.736G       0.989         6.6       0.195       0.247: 100% 372/372 [00:39<00:00,  9.44it/s]\n",
            "    35/100    0.736G       0.993        6.61       0.195       0.252: 100% 372/372 [00:40<00:00,  9.10it/s]\n",
            "    36/100    0.736G       0.984        6.65       0.195       0.248: 100% 372/372 [00:39<00:00,  9.37it/s]\n",
            "    37/100    0.736G       0.983        6.65       0.195       0.253: 100% 372/372 [00:39<00:00,  9.43it/s]\n",
            "    38/100    0.736G       0.977        6.67       0.195       0.254: 100% 372/372 [00:40<00:00,  9.25it/s]\n",
            "    39/100    0.736G       0.972        6.69       0.195       0.248: 100% 372/372 [00:39<00:00,  9.45it/s]\n",
            "    40/100    0.736G       0.969        6.69       0.195       0.243: 100% 372/372 [00:40<00:00,  9.21it/s]\n",
            "    41/100    0.736G       0.969        6.67       0.195       0.246: 100% 372/372 [00:38<00:00,  9.64it/s]\n",
            "    42/100    0.736G       0.962        6.67       0.195       0.246: 100% 372/372 [00:40<00:00,  9.19it/s]\n",
            "    43/100    0.736G       0.958        6.65       0.195        0.25: 100% 372/372 [00:38<00:00,  9.54it/s]\n",
            "    44/100    0.736G       0.966        6.64       0.195        0.25: 100% 372/372 [00:40<00:00,  9.10it/s]\n",
            "    45/100    0.736G       0.955        6.65       0.195       0.248: 100% 372/372 [00:38<00:00,  9.64it/s]\n",
            "    46/100    0.736G       0.948        6.64       0.195       0.248: 100% 372/372 [00:40<00:00,  9.09it/s]\n",
            "    47/100    0.736G        0.95        6.64       0.195        0.25: 100% 372/372 [00:39<00:00,  9.42it/s]\n",
            "    48/100    0.736G       0.946        6.63       0.195        0.25: 100% 372/372 [00:40<00:00,  9.12it/s]\n",
            "    49/100    0.736G       0.939        6.62       0.195       0.249: 100% 372/372 [00:41<00:00,  8.97it/s]\n",
            "    50/100    0.736G        0.93        6.62       0.195       0.249: 100% 372/372 [00:39<00:00,  9.40it/s]\n",
            "    51/100    0.736G       0.934        6.62       0.195        0.25: 100% 372/372 [00:41<00:00,  8.94it/s]\n",
            "    52/100    0.736G       0.933         6.6       0.195       0.248: 100% 372/372 [00:39<00:00,  9.48it/s]\n",
            "    53/100    0.736G       0.936        6.59       0.195       0.248: 100% 372/372 [00:41<00:00,  9.01it/s]\n",
            "    54/100    0.736G       0.931         6.6       0.195       0.245: 100% 372/372 [00:39<00:00,  9.54it/s]\n",
            "    55/100    0.736G        0.92        6.58       0.195       0.244: 100% 372/372 [00:40<00:00,  9.08it/s]\n",
            "    56/100    0.736G        0.92        6.57       0.195       0.245: 100% 372/372 [00:40<00:00,  9.28it/s]\n",
            "    57/100    0.736G       0.914        6.57       0.195       0.245: 100% 372/372 [00:40<00:00,  9.27it/s]\n",
            "    58/100    0.736G       0.916        6.55       0.195       0.246: 100% 372/372 [00:41<00:00,  9.05it/s]\n",
            "    59/100    0.736G       0.918        6.53       0.195       0.248: 100% 372/372 [00:38<00:00,  9.58it/s]\n",
            "    60/100    0.736G        0.91        6.53       0.195       0.248: 100% 372/372 [00:41<00:00,  9.03it/s]\n",
            "    61/100    0.736G       0.906        6.52       0.195       0.248: 100% 372/372 [00:38<00:00,  9.70it/s]\n",
            "    62/100    0.736G       0.912        6.51       0.195       0.247: 100% 372/372 [00:40<00:00,  9.24it/s]\n",
            "    63/100    0.736G       0.905         6.5       0.195       0.249: 100% 372/372 [00:38<00:00,  9.75it/s]\n",
            "    64/100    0.736G       0.905        6.49       0.195       0.249: 100% 372/372 [00:40<00:00,  9.18it/s]\n",
            "    65/100    0.736G       0.904        6.47       0.195       0.249: 100% 372/372 [00:38<00:00,  9.70it/s]\n",
            "    66/100    0.736G         0.9        6.46       0.195       0.249: 100% 372/372 [00:39<00:00,  9.30it/s]\n",
            "    67/100    0.736G       0.898        6.45       0.195       0.247: 100% 372/372 [00:38<00:00,  9.70it/s]\n",
            "    68/100    0.736G       0.897        6.45       0.195       0.247: 100% 372/372 [00:40<00:00,  9.21it/s]\n",
            "    69/100    0.736G       0.896        6.44       0.195       0.245: 100% 372/372 [00:38<00:00,  9.70it/s]\n",
            "    70/100    0.736G       0.894        6.41       0.195       0.244: 100% 372/372 [00:40<00:00,  9.24it/s]\n",
            "    71/100    0.736G       0.891        6.41       0.195       0.243: 100% 372/372 [00:38<00:00,  9.58it/s]\n",
            "    72/100    0.736G        0.89        6.39       0.195       0.243: 100% 372/372 [00:40<00:00,  9.24it/s]\n",
            "    73/100    0.736G       0.888        6.37       0.195       0.242: 100% 372/372 [00:40<00:00,  9.30it/s]\n",
            "    74/100    0.736G       0.885        6.35       0.195        0.24: 100% 372/372 [00:39<00:00,  9.45it/s]\n",
            "    75/100    0.736G       0.881        6.33       0.195        0.24: 100% 372/372 [00:39<00:00,  9.39it/s]\n",
            "    76/100    0.736G       0.877        6.33       0.195       0.239: 100% 372/372 [00:39<00:00,  9.53it/s]\n",
            "    77/100    0.736G        0.88         6.3       0.195        0.24: 100% 372/372 [00:39<00:00,  9.42it/s]\n",
            "    78/100    0.736G       0.877        6.29       0.195        0.24: 100% 372/372 [00:38<00:00,  9.60it/s]\n",
            "    79/100    0.736G       0.872        6.28       0.195       0.239: 100% 372/372 [00:39<00:00,  9.41it/s]\n",
            "    80/100    0.736G       0.873        6.25       0.195       0.236: 100% 372/372 [00:39<00:00,  9.48it/s]\n",
            "    81/100    0.736G       0.869        6.25       0.195       0.232: 100% 372/372 [00:39<00:00,  9.31it/s]\n",
            "    82/100    0.736G       0.869        6.22       0.195       0.232: 100% 372/372 [00:38<00:00,  9.63it/s]\n",
            "    83/100    0.736G       0.862        6.21       0.195       0.231: 100% 372/372 [00:39<00:00,  9.42it/s]\n",
            "    84/100    0.736G       0.861         6.2       0.195       0.231: 100% 372/372 [00:38<00:00,  9.61it/s]\n",
            "    85/100    0.736G       0.862        6.19       0.195        0.23: 100% 372/372 [00:40<00:00,  9.28it/s]\n",
            "    86/100    0.736G        0.86        6.17       0.195       0.227: 100% 372/372 [00:39<00:00,  9.53it/s]\n",
            "    87/100    0.736G       0.861        6.16       0.195       0.228: 100% 372/372 [00:40<00:00,  9.09it/s]\n",
            "    88/100    0.736G       0.858        6.14       0.195       0.226: 100% 372/372 [00:38<00:00,  9.66it/s]\n",
            "    89/100    0.736G       0.855        6.12       0.195       0.225: 100% 372/372 [00:40<00:00,  9.28it/s]\n",
            "    90/100    0.736G       0.855         6.1       0.195       0.225: 100% 372/372 [00:37<00:00,  9.79it/s]\n",
            "    91/100    0.736G       0.855        6.08       0.195       0.223: 100% 372/372 [00:40<00:00,  9.24it/s]\n",
            "    92/100    0.736G       0.849        6.06       0.195       0.222: 100% 372/372 [00:38<00:00,  9.78it/s]\n",
            "    93/100    0.736G       0.848        6.06       0.195       0.224: 100% 372/372 [00:40<00:00,  9.22it/s]\n",
            "    94/100    0.736G       0.849        6.06       0.195       0.221: 100% 372/372 [00:38<00:00,  9.76it/s]\n",
            "    95/100    0.736G       0.846        6.04       0.195       0.221: 100% 372/372 [00:39<00:00,  9.32it/s]\n",
            "    96/100    0.736G       0.843        6.02       0.195        0.22: 100% 372/372 [00:38<00:00,  9.71it/s]\n",
            "    97/100    0.736G       0.842        6.03       0.195       0.219: 100% 372/372 [00:40<00:00,  9.14it/s]\n",
            "    98/100    0.736G        0.84           6       0.195       0.219: 100% 372/372 [00:38<00:00,  9.71it/s]\n",
            "    99/100    0.736G        0.84           6       0.195       0.219: 100% 372/372 [00:39<00:00,  9.39it/s]\n",
            "   100/100    0.736G       0.838        5.97       0.195       0.219: 100% 372/372 [00:38<00:00,  9.62it/s]\n",
            "\n",
            "Training complete (1.109 hours)\n",
            "Results saved to \u001b[1m../content/yolov5/runs/train-cls/exp\u001b[0m\n",
            "Predict:         python classify/predict.py --weights ../content/yolov5/runs/train-cls/exp/weights/best.pt --source im.jpg\n",
            "Validate:        python classify/val.py --weights ../content/yolov5/runs/train-cls/exp/weights/best.pt --data /content/datasets/Sign-Language-Detectiom-1\n",
            "Export:          python export.py --weights ../content/yolov5/runs/train-cls/exp/weights/best.pt --include onnx\n",
            "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '../content/yolov5/runs/train-cls/exp/weights/best.pt')\n",
            "Visualize:       https://netron.app\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate Your Custom Model\n",
        "\n",
        "Repeat step 2 from above to test and validate your custom model."
      ],
      "metadata": {
        "id": "HHUFGeLbGd98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/yolov5/classify/val.py --weights /content/yolov5/runs/train-cls/exp/weights/best.pt --data ../datasets/$DATASET_NAME"
      ],
      "metadata": {
        "id": "DIV7ydyKGZFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dddb8bd-b73a-4de4-cac5-14047e30eab2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mclassify/val: \u001b[0mdata=../datasets/Sign-Language-Detectiom-1, weights=['/content/yolov5/runs/train-cls/exp/weights/best.pt'], batch_size=128, imgsz=224, device=, workers=8, verbose=True, project=../content/yolov5/runs/val-cls, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v7.0-245-g3d8f004 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 4293507 parameters, 0 gradients, 10.5 GFLOPs\n",
            "testing: 100% 8/8 [00:01<00:00,  4.04it/s]\n",
            "                   Class      Images    top1_acc    top5_acc\n",
            "                     all         997       0.179       0.224\n",
            "                       1          20           1           1\n",
            "                       2          15           1           1\n",
            "                       3          23       0.261           1\n",
            "                       4          21           1           1\n",
            "                       5          24           1           1\n",
            "                       6          12           1           1\n",
            "                       7          17           1           1\n",
            "                       8          20           1           1\n",
            "                       9          19           1           1\n",
            "                       A          11           1           1\n",
            "                All_Gone           1           0           0\n",
            "                       B          10           1           1\n",
            "                    Baby           2           0           0\n",
            "                  Beside           1           0           0\n",
            "                    Book           3       0.333       0.333\n",
            "                    Bowl           3           0           0\n",
            "                  Bridge          11           0           0\n",
            "                       C          31           0           0\n",
            "                    Camp          14           0           0\n",
            "               Cartridge          17           0           0\n",
            "                       D           2           0           0\n",
            "                       E           1           0           0\n",
            "                       F          20           0           0\n",
            "                    Fond           2           0         0.5\n",
            "                  Friend          20           0           0\n",
            "                       G           1           0           0\n",
            "                   Glove           2           0           0\n",
            "                       H           4           0           0\n",
            "                    Hang          20           0           0\n",
            "                    High          27           0           0\n",
            "                   House          17           0           0\n",
            "                How_Many          13           0           0\n",
            "                       I          20           0           0\n",
            "                   IorMe           5           0           0\n",
            "                       J           2           0           0\n",
            "                       K           4           0           0\n",
            "                       L           1           0           0\n",
            "                       M           1           0           0\n",
            "                     Man           3           0           0\n",
            "                   Marry           2           0           0\n",
            "                    Meat           1           0           0\n",
            "                   Medal          19           0           0\n",
            "                 Mid_Day          23           0           0\n",
            "                  Middle           2           0           0\n",
            "                   Money          20           0           0\n",
            "                    Moon          19           0           0\n",
            "                  Mother          16           0           0\n",
            "                       N           1           0           0\n",
            "                       O          19           0       0.211\n",
            "                Opposite           4           0           0\n",
            "                       P           2           0           0\n",
            "                Prisoner           1           0           0\n",
            "                       Q          28           0           0\n",
            "                       R           2           0           0\n",
            "                    Ring           1           0           0\n",
            "                    Rose           2           0           0\n",
            "                       S          15           0           0\n",
            "                     See           1           0           0\n",
            "                   Short           1           0           0\n",
            "                Superior           3           0           0\n",
            "                       T           2           0           0\n",
            "                   Thick           3           0           0\n",
            "                    Thin           1           0           0\n",
            "                 Tobacco          17           0           0\n",
            "                       U          23           0       0.174\n",
            "                      Up           1           0           0\n",
            "                  User_1           2           0           0\n",
            "                  User_2          12           0           0\n",
            "                  User_3          16           0           0\n",
            "                  User_4          18           0           0\n",
            "                  User_5           4           0           0\n",
            "                  User_6           6           0           0\n",
            "                       V          18           0           0\n",
            "                       W          10           0           0\n",
            "                   Watch          14           0           0\n",
            "                   Write          13           0           0\n",
            "                       X           7           0           0\n",
            "                       Y          15           0           0\n",
            "                       Z          11           0           0\n",
            "                  aboard          15           0           0\n",
            "                  afraid          19           0           0\n",
            "                   agree          16           0           0\n",
            "              assistance          17           0           0\n",
            "                  become          17      0.0588       0.353\n",
            "                 college          19           0       0.105\n",
            "                  doctor          16      0.0625       0.312\n",
            "                    from          10           0           0\n",
            "                    pain          15           0           0\n",
            "                    pray          25           0        0.12\n",
            "               secondary          13           0       0.385\n",
            "                    skin           0         nan         nan\n",
            "                   small           0         nan         nan\n",
            "                specific           0         nan         nan\n",
            "                   stand           0         nan         nan\n",
            "                   today           0         nan         nan\n",
            "                    warn           0         nan         nan\n",
            "                   which           0         nan         nan\n",
            "                    work           0         nan         nan\n",
            "                     you           0         nan         nan\n",
            "Speed: 0.1ms pre-process, 1.1ms inference, 0.0ms post-process per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1m../content/yolov5/runs/val-cls/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Infer With Your Custom Model"
      ],
      "metadata": {
        "id": "uH5tJNpEsi6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the path of an image from the test or validation set\n",
        "if os.path.exists(os.path.join(dataset.location, \"test\")):\n",
        "  split_path = os.path.join(dataset.location, \"test\")\n",
        "else:\n",
        "  os.path.join(dataset.location, \"valid\")\n",
        "example_class = os.listdir(split_path)[0]\n",
        "example_image_name = os.listdir(os.path.join(split_path, example_class))[0]\n",
        "example_image_path = os.path.join(split_path, example_class, example_image_name)\n",
        "os.environ[\"TEST_IMAGE_PATH\"] = example_image_path\n",
        "\n",
        "print(f\"Inferring on an example of the class '{example_class}'\")\n",
        "\n",
        "#Infer\n",
        "!python /content/yolov5/classify/predict.py --weights /content/yolov5/runs/train-cls/exp/weights/best.pt --source $TEST_IMAGE_PATH"
      ],
      "metadata": {
        "id": "81lK1hU_sk54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7760cc1-9ed3-4f0e-86ac-efe56ff4b661"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inferring on an example of the class 'M'\n",
            "\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['/content/yolov5/runs/train-cls/exp/weights/best.pt'], source=/datasets/Sign-Language-Detectiom-1/test/M/518_jpg.rf.ff674d0de38ff34ee63eb489650283a5.jpg, data=../content/yolov5/data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=False, nosave=False, augment=False, visualize=False, update=False, project=../content/yolov5/runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 ðŸš€ v7.0-245-g3d8f004 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 4293507 parameters, 0 gradients, 10.5 GFLOPs\n",
            "image 1/1 /datasets/Sign-Language-Detectiom-1/test/M/518_jpg.rf.ff674d0de38ff34ee63eb489650283a5.jpg: 224x224 M 0.30, L 0.05, N 0.04, Z 0.03, H 0.03, 2.8ms\n",
            "Speed: 0.3ms pre-process, 2.8ms inference, 0.2ms NMS per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1m../content/yolov5/runs/predict-cls/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the inference results show ~3ms inference and the respective classes predicted probabilities."
      ],
      "metadata": {
        "id": "DdGuG-1kNjWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "TIsQ6jagBlyS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (OPTIONAL) Improve Our Model with Active Learning\n",
        "\n",
        "Now that we've trained our model once, we will want to continue to improve its performance. Improvement is largely dependent on improving our dataset.\n",
        "\n",
        "We can programmatically upload example failure images back to our custom dataset based on conditions (like seeing an underrpresented class or a low confidence score) using the same `pip` package."
      ],
      "metadata": {
        "id": "I38IM6NXKNN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python /content/yolov5/classify/predict.py --weights /content/yolov5/runs/train-cls/exp/weights/best.pt --source /content/Sign-Language-Detectiom-1/test/C/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU01SzV3B0hN",
        "outputId": "d559e643-4c30-4cc8-99b2-9b1f790825e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['/content/yolov5/runs/train-cls/exp/weights/best.pt'], source=/content/Sign-Language-Detectiom-1/test/C/, data=../content/yolov5/data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=False, nosave=False, augment=False, visualize=False, update=False, project=../content/yolov5/runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 ðŸš€ v7.0-245-g3d8f004 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 4293507 parameters, 0 gradients, 10.5 GFLOPs\n",
            "image 1/11 /content/Sign-Language-Detectiom-1/test/C/014_jpg.rf.05d55286ecce341e60e8aba3106beb4f.jpg: 224x224 O 0.40, V 0.21, C 0.13, I 0.03, Book 0.02, 4.4ms\n",
            "image 2/11 /content/Sign-Language-Detectiom-1/test/C/1093_jpg.rf.b835e658765b9c6e24d0d1c26970039c.jpg: 224x224 C 0.21, Y 0.09, T 0.07, I 0.06, O 0.05, 4.6ms\n",
            "image 3/11 /content/Sign-Language-Detectiom-1/test/C/131_jpg.rf.65da1d9a57649cfd0b8671a5b651a420.jpg: 224x224 C 0.20, Y 0.15, I 0.12, W 0.05, O 0.04, 3.9ms\n",
            "image 4/11 /content/Sign-Language-Detectiom-1/test/C/132_jpg.rf.181403e608f22be8524c947459eb78fe.jpg: 224x224 C 0.19, Y 0.15, I 0.11, W 0.05, O 0.05, 4.1ms\n",
            "image 5/11 /content/Sign-Language-Detectiom-1/test/C/441_jpg.rf.eee9f50ba95c3824737034533c8b2a2b.jpg: 224x224 C 0.16, Y 0.13, I 0.11, O 0.06, W 0.05, 4.0ms\n",
            "image 6/11 /content/Sign-Language-Detectiom-1/test/C/562_jpg.rf.636fa0dafe5aa54003cc16762596af51.jpg: 224x224 C 0.15, I 0.13, Y 0.10, O 0.07, W 0.04, 3.6ms\n",
            "image 7/11 /content/Sign-Language-Detectiom-1/test/C/629_jpg.rf.90885eb7a4068f26f74afe26a240624d.jpg: 224x224 C 0.19, I 0.13, Y 0.11, O 0.06, W 0.04, 3.8ms\n",
            "image 8/11 /content/Sign-Language-Detectiom-1/test/C/833_jpg.rf.30c3bf5db0dfaee051c4d21f4053ef71.jpg: 224x224 C 0.23, I 0.14, Y 0.11, W 0.04, O 0.04, 3.7ms\n",
            "image 9/11 /content/Sign-Language-Detectiom-1/test/C/910_jpg.rf.8234cc3ee99a7f735fdc3cc94262ab6d.jpg: 224x224 C 0.25, Y 0.07, I 0.07, T 0.06, O 0.05, 3.9ms\n",
            "image 10/11 /content/Sign-Language-Detectiom-1/test/C/926_jpg.rf.96961eb1a6909899d081c625d75043ff.jpg: 224x224 C 0.23, I 0.07, Y 0.07, T 0.06, O 0.05, 3.7ms\n",
            "image 11/11 /content/Sign-Language-Detectiom-1/test/C/934_jpg.rf.05b9cfbf09739dda90e2eadec5c65ada.jpg: 224x224 C 0.16, Y 0.14, I 0.12, O 0.06, T 0.05, 3.7ms\n",
            "Speed: 0.3ms pre-process, 3.9ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1m../content/yolov5/runs/predict-cls/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-riUEaXhB0iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Upload example image\n",
        "# project.upload(image_path)\n"
      ],
      "metadata": {
        "id": "HycgSEnYKo0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example upload code\n",
        "# min_conf = float(\"inf\")\n",
        "# for pred in results:\n",
        "#     if pred[\"score\"] < min_conf:\n",
        "#         min_conf = pred[\"score\"]\n",
        "# if min_conf < 0.4:\n",
        "#     project.upload(image_path)"
      ],
      "metadata": {
        "id": "VwXDoz_vLK3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (BONUS) YOLOv5 classify/predict.py Accepts Several Input Methods\n",
        "- Webcam: `python classify/predict.py --weights yolov5s-cls.pt --source 0`\n",
        "- Image `python classify/predict.py --weights yolov5s-cls.pt --source img.jpg`\n",
        "- Video: `python classify/predict.py --weights yolov5s-cls.pt --source vid.mp4`\n",
        "- Directory: `python classify/predict.py --weights yolov5s-cls.pt --source path/`\n",
        "- Glob: `python classify/predict.py --weights yolov5s-cls.pt --source 'path/*.jpg'`\n",
        "- YouTube: `python classify/predict.py --weights yolov5s-cls.pt --source 'https://youtu.be/Zgi9g1ksQHc'`\n",
        "- RTSP, RTMP, HTTP stream: `python classify/predict.py --weights yolov5s-cls.pt --source 'rtsp://example.com/media.mp4'`"
      ],
      "metadata": {
        "id": "aYlfaHDusN-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Directory Example"
      ],
      "metadata": {
        "id": "iKSP-SNTvcLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Directory infer\n",
        "os.environ[\"TEST_CLASS_PATH\"] = test_class_path = os.path.join(*os.environ[\"TEST_IMAGE_PATH\"].split(os.sep)[:-1])\n",
        "print(f\"Infering on all images from the directory {os.environ['TEST_CLASS_PATH']}\")\n",
        "!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source /$TEST_CLASS_PATH/"
      ],
      "metadata": {
        "id": "lwSoHcHcvjeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###YouTube Example"
      ],
      "metadata": {
        "id": "kCCao9t8se8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YouTube infer\n",
        "!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source 'https://www.youtube.com/watch?v=7AlYA4ItA74'"
      ],
      "metadata": {
        "id": "heebjpJBsakV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}